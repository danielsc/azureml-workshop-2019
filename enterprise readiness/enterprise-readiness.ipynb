{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML end to end enterprise readiness scenario\n",
    "\n",
    "This notebook shows the steps for end to end enterprise readiness scenario for AML i.e. training and inference in a secure manner. Please make sure you have an [AML workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace#create-a-workspace) before you proceed with this notebook. We will cover following steps in this notebook:\n",
    "\n",
    "1. Create a virtual network (VNet)\n",
    "2. Setup Network Security Group (NSG) rules\n",
    "3. Create Machine Learning Compute inside the VNet\n",
    "4. Create AKS cluster inside the VNet\n",
    "5. Create a DSVM inside the VNet\n",
    "6. Put storage and key vault behind the VNet\n",
    "7. Train on AML Compute inside the VNet (secure training)\n",
    "8. Deploy model as a service on AKS cluster using SSL and auth settings (secure inference)\n",
    "9. Consume the service in a secure manner (using SSL and auth keys/token)\n",
    "10. Setup role based access for your workspace\n",
    "11. Monitor e2e ML lifecycle and set alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a virtual network (VNet)\n",
    "\n",
    "Create a VNet with a subnet as shown in the image below. Make sure VNet is linked to NSG as shown in the screenshot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Network Security Group (NSG) rules\n",
    "\n",
    "NSG rules should be set as shown in the screenshots below:\n",
    "\n",
    "<img src=\"nsg-rules.png\" alt=\"Screenshot showing NSG rules\" title=\"NSG rules\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Machine Learning Compute inside the VNet\n",
    "\n",
    "Use the code snippet below or UI (screenshot below) to create AML Compute inside the VNet/Subnet created above\n",
    "\n",
    "<img src=\"aml-compute-creation.png\" alt=\"Image showing AML Compute creation behind VNet\" title=\"Create AML Compute behind VNet\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# initialize the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# The Azure virtual network name, subnet, and resource group\n",
    "vnet_name = 'AMLVNetDemo'\n",
    "subnet_name = 'default'\n",
    "vnet_resourcegroup_name = 'AMLServiceRG'\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"vnetcluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print(\"Found existing cpucluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cpucluster\")\n",
    "\n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\",\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4,\n",
    "                                                           vnet_resourcegroup_name=vnet_resourcegroup_name,\n",
    "                                                           vnet_name=vnet_name,\n",
    "                                                           subnet_name=subnet_name)\n",
    "\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "    # Wait for the cluster to be completed, show the output log\n",
    "    cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create AKS cluster inside the VNet\n",
    "\n",
    "Use the code snippet below or UI (screenshot below) to create AKS cluster inside the VNet/Subnet created above\n",
    "\n",
    "<img src=\"aks-cluster-creation.png\" alt=\"Image showing AKS cluster creation behind VNet\" title=\"Create AKS cluster behind VNet\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "aks_cluster_name = 'aksvnetcluster1'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_cluster_name)\n",
    "    print(\"Found existing aks cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new aks cluster\")\n",
    "\n",
    "    # Create the compute configuration and set virtual network information\n",
    "    config = AksCompute.provisioning_configuration(location=\"eastus\")\n",
    "    config.vnet_resourcegroup_name = \"AMLServiceRG\"\n",
    "    config.vnet_name = \"AMLVNetDemo\"\n",
    "    config.subnet_name = \"default\"\n",
    "    config.service_cidr = \"10.0.0.0/16\"\n",
    "    config.dns_service_ip = \"10.0.0.10\"\n",
    "    config.docker_bridge_cidr = \"172.17.0.1/16\"\n",
    "\n",
    "    # Create the compute target\n",
    "    aks_target = ComputeTarget.create(workspace=ws,\n",
    "                                      name=aks_cluster_name,\n",
    "                                      provisioning_configuration=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_target.wait_for_completion(show_output = True)\n",
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable SSL on the AKS Cluster (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring your own SSL cert\n",
    "# provisioning_config.enable_ssl(ssl_cert_pem_file=\"cert.pem\",\n",
    "#                                     ssl_key_pem_file=\"key.pem\", ssl_cname=\"www.contoso.com\")\n",
    "\n",
    "# use a cert from Microsoft\n",
    "# provisioning_config.enable_ssl(leaf_domain_label = \"myservice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a DSVM inside the VNet\n",
    "\n",
    "Follow instructions [here](https://docs.microsoft.com/en-us/azure/virtual-network/quick-create-portal) to create a DSVM inside the same VNet/Subnet we created above. Once VM is created setup a dev environment on the VM so that you can use jupyter notebooks from inside the VM. These jupyter notebooks will be user to run training and inference inside the VNet.\n",
    "\n",
    "NOTE: This experience will soon be replaced by managed compute instances that will work behind the VNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Up to this point we have created VNet/Subnet, setup NSG rules, created AML Compute (training compute target), AKS cluster (inference compute target), and DSVM (dev environment) behind the VNet. We are now ready to do end to end ML behind the VNet.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Put storage and key vault behind the VNet\n",
    "\n",
    "Follow the links to put [storage](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-enable-virtual-network#use-a-storage-account-for-your-workspace) and [key vault](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-enable-virtual-network#use-a-key-vault-instance-with-your-workspace) that are associated with the AML workspace behind the VNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train on AML Compute inside the VNet (secure training)\n",
    "\n",
    "We will now train a model inside the VNet/Subnet created above. We will use DSVM running inside the VNet as our dev environment. This DSVM will use AML python SDK to execute training jobs on AML compute that is running inside the same VNet as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experiment\n",
    "from azureml.core import Experiment\n",
    "experiment_name = 'train-on-amlcompute'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# create a project directory\n",
    "project_folder = './train-on-amlcompute'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy('train.py', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "pip_packages = [\n",
    "    'azureml-defaults', 'azureml-contrib-explain-model', 'azureml-core', 'azureml-telemetry',\n",
    "    'azureml-explain-model', 'sklearn-pandas', 'azureml-dataprep'\n",
    "]\n",
    "\n",
    "estimator = Estimator(source_directory=project_folder, \n",
    "                      compute_target=cpu_cluster,\n",
    "                      entry_script='train.py',\n",
    "                      pip_packages=pip_packages,\n",
    "                      conda_packages=['scikit-learn'],\n",
    "                      inputs=[ws.datasets['IBM-Employee-Attrition'].as_named_input('attrition')])\n",
    "\n",
    "run = experiment.submit(estimator)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploy model as a service on AKS cluster using SSL and auth (secure inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to deploy\n",
    "\n",
    "To deploy the model, you need the following items:\n",
    "\n",
    "- **An entry script**, this script accepts requests, scores the requests by using the model, and returns the results.\n",
    "- **Dependencies**, like helper scripts or Python/Conda packages required to run the entry script or model.\n",
    "- **The deployment configuration** for the compute target that hosts the deployed model. This configuration describes things like memory and CPU requirements needed to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
    "\n",
    "\n",
    "input_sample = pd.DataFrame(data=[{'Age': 41, 'BusinessTravel': 'Travel_Rarely', 'DailyRate': 1102, 'Department': 'Sales', 'DistanceFromHome': 1, 'Education': 2, 'EducationField': 'Life Sciences', 'EnvironmentSatisfaction': 2, 'Gender': 'Female', 'HourlyRate': 94, 'JobInvolvement': 3, 'JobLevel': 2, 'JobRole': 'Sales Executive', 'JobSatisfaction': 4, 'MaritalStatus': 'Single', 'MonthlyIncome': 5993, 'MonthlyRate': 19479, 'NumCompaniesWorked': 8, 'OverTime': 'No', 'PercentSalaryHike': 11, 'PerformanceRating': 3, 'RelationshipSatisfaction': 1, 'StockOptionLevel': 0, 'TotalWorkingYears': 8, 'TrainingTimesLastYear': 0, 'WorkLifeBalance': 1, 'YearsAtCompany': 6, 'YearsInCurrentRole': 4, 'YearsSinceLastPromotion': 0, 'YearsWithCurrManager': 5}])\n",
    "output_sample = np.array([0])\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # This name is model.id of model that we want to deploy deserialize the model file back\n",
    "    # into a sklearn model\n",
    "    model_path = Model.get_model_path('IBM_attrition_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "@input_schema('data', PandasParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "def run(data):\n",
    "    try:\n",
    "        result = model.predict(data)\n",
    "        return json.dumps({\"result\": result.tolist()})\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic schema generation\n",
    "To automatically generate a schema for your web service, provide a sample of the input and/or output in the constructor for one of the defined type objects. The type and sample are used to automatically create the schema. Azure Machine Learning then creates an OpenAPI (Swagger) specification for the web service during deployment.\n",
    "To use schema generation, include the _inference-schema_ package in your Conda environment file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dependencies\n",
    "\n",
    "The following YAML is the Conda dependencies file we will use for inference. If you want to use automatic schema generation, your entry script must import the inference-schema packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile myenv.yml\n",
    "\n",
    "name: project_environment\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "\n",
    "- pip:\n",
    "  - sklearn-pandas\n",
    "  - azureml-defaults\n",
    "  - azureml-core\n",
    "  - inference-schema[numpy-support]\n",
    "- scikit-learn\n",
    "- pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Instantiate environment\n",
    "myenv = Environment.from_conda_specification(name = \"myenv\",\n",
    "                                             file_path = \"myenv.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your inference configuration\n",
    "\n",
    "The inference configuration describes how to configure the model to make predictions. This configuration isn't part of your entry script. It references your entry script and is used to locate all the resources required by the deployment. It's used later, when you deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your deployment configuration\n",
    "\n",
    "Before deploying your model, you must define the deployment configuration. The deployment configuration is specific to the compute target that will host the web service. The deployment configuration isn't part of your entry script. It's used to define the characteristics of the compute target that will host the model and entry script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "\n",
    "# Set the web service configuration (using default here)\n",
    "# aks_config = AksWebservice.deploy_configuration(tags={'area': \"HR\", 'type': \"attrition\"}, \n",
    "#                                               description='Explain predictions on employee attrition')\n",
    "\n",
    "# Enable token auth and disable (key) auth on the webservice\n",
    "aks_config = AksWebservice.deploy_configuration(tags={'area': \"HR\", 'type': \"attrition\"}, \n",
    "                                               description='Explain predictions on employee attrition', \n",
    "                                                token_auth_enabled=True, auth_enabled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Deploy Model as Webservice on AKS cluster\n",
    "\n",
    "Deployment uses the inference configuration deployment configuration to deploy the models. The deployment process is similar regardless of the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "aks_service_name ='aks-attrition-svc'\n",
    "\n",
    "attrition_model = Model(ws, 'IBM_attrition_model')\n",
    "\n",
    "aks_service = Model.deploy(workspace=ws,\n",
    "                           name=aks_service_name,\n",
    "                           models=[attrition_model],\n",
    "                           inference_config=inference_config,\n",
    "                           deployment_config=aks_config,\n",
    "                           deployment_target=aks_target)\n",
    "\n",
    "\n",
    "aks_service.wait_for_deployment(True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web service schema\n",
    "\n",
    "If you used automatic schema generation with your deployment, you can get the address of the OpenAPI specification for the service by using the swagger_uri property. (For example, print(service.swagger_uri).) Use a GET request or open the URI in a browser to retrieve the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aks_service.swagger_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Consume the service in a secure manner (using SSL and auth keys/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if (key) auth is enabled, retrieve the API keys. AML generates two keys.\n",
    "# key1, Key2 = aks_service.get_keys()\n",
    "# print(key1)\n",
    "\n",
    "# if token auth is enabled, retrieve the token.\n",
    "access_token, refresh_after = aks_service.get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct raw HTTP request and send to the service\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "\n",
    "import json\n",
    "\n",
    "# the sample below contains the data for an employee that is not an attrition risk\n",
    "sample = pd.DataFrame(data=[{'Age': 41, 'BusinessTravel': 'Travel_Rarely', 'DailyRate': 1102, 'Department': 'Sales', 'DistanceFromHome': 1, 'Education': 2, 'EducationField': 'Life Sciences', 'EnvironmentSatisfaction': 2, 'Gender': 'Female', 'HourlyRate': 94, 'JobInvolvement': 3, 'JobLevel': 2, 'JobRole': 'Sales Executive', 'JobSatisfaction': 4, 'MaritalStatus': 'Single', 'MonthlyIncome': 5993, 'MonthlyRate': 19479, 'NumCompaniesWorked': 8, 'OverTime': 0, 'PercentSalaryHike': 11, 'PerformanceRating': 3, 'RelationshipSatisfaction': 1, 'StockOptionLevel': 0, 'TotalWorkingYears': 8, 'TrainingTimesLastYear': 0, 'WorkLifeBalance': 1, 'YearsAtCompany': 6, 'YearsInCurrentRole': 4, 'YearsSinceLastPromotion': 0, 'YearsWithCurrManager': 5}])\n",
    "\n",
    "# the sample below contains the data for an employee that is an attrition risk\n",
    "# sample = pd.DataFrame(data=[{'Age': 49, 'BusinessTravel': 'Travel_Rarely', 'DailyRate': 1098, 'Department': 'Research & Development', 'DistanceFromHome': 4, 'Education': 2, 'EducationField': 'Medical', 'EnvironmentSatisfaction': 4, 'Gender': 'Female', 'HourlyRate': 21, 'JobInvolvement': 3, 'JobLevel': 2, 'JobRole': 'Laboratory Technician', 'JobSatisfaction': 3, 'MaritalStatus': 'Single', 'MonthlyIncome': 711, 'MonthlyRate': 2124, 'NumCompaniesWorked': 8, 'OverTime': 1, 'PercentSalaryHike': 8, 'PerformanceRating': 4, 'RelationshipSatisfaction': 3, 'StockOptionLevel': 0, 'TotalWorkingYears': 2, 'TrainingTimesLastYear': 0, 'WorkLifeBalance': 3, 'YearsAtCompany': 2, 'YearsInCurrentRole': 1, 'YearsSinceLastPromotion': 0, 'YearsWithCurrManager': 1}])\n",
    "\n",
    "# converts the sample to JSON string\n",
    "sample = pd.DataFrame.to_json(sample)\n",
    "\n",
    "# deserializes sample to a python object \n",
    "sample = json.loads(sample)\n",
    "\n",
    "# serializes sample to JSON formatted string as expected by the scoring script\n",
    "sample = json.dumps({\"data\":sample})\n",
    "\n",
    "# If (key) auth is enabled, don't forget to add key to the HTTP header.\n",
    "# headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + key1}\n",
    "\n",
    "# If token auth is enabled, don't forget to add token to the HTTP header.\n",
    "headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + access_token}\n",
    "\n",
    "resp = requests.post(aks_service.scoring_uri, sample, headers=headers)\n",
    "\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Use run function that internally handles auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# the sample below contains the data for an employee that is not an attrition risk\n",
    "sample = pd.DataFrame(data=[{'Age': 41, 'BusinessTravel': 'Travel_Rarely', 'DailyRate': 1102, 'Department': 'Sales', 'DistanceFromHome': 1, 'Education': 2, 'EducationField': 'Life Sciences', 'EnvironmentSatisfaction': 2, 'Gender': 'Female', 'HourlyRate': 94, 'JobInvolvement': 3, 'JobLevel': 2, 'JobRole': 'Sales Executive', 'JobSatisfaction': 4, 'MaritalStatus': 'Single', 'MonthlyIncome': 5993, 'MonthlyRate': 19479, 'NumCompaniesWorked': 8, 'OverTime': 0, 'PercentSalaryHike': 11, 'PerformanceRating': 3, 'RelationshipSatisfaction': 1, 'StockOptionLevel': 0, 'TotalWorkingYears': 8, 'TrainingTimesLastYear': 0, 'WorkLifeBalance': 1, 'YearsAtCompany': 6, 'YearsInCurrentRole': 4, 'YearsSinceLastPromotion': 0, 'YearsWithCurrManager': 5}])\n",
    "\n",
    "# the sample below contains the data for an employee that is an attrition risk\n",
    "# sample = pd.DataFrame(data=[{'Age': 49, 'BusinessTravel': 'Travel_Rarely', 'DailyRate': 1098, 'Department': 'Research & Development', 'DistanceFromHome': 4, 'Education': 2, 'EducationField': 'Medical', 'EnvironmentSatisfaction': 4, 'Gender': 'Female', 'HourlyRate': 21, 'JobInvolvement': 3, 'JobLevel': 2, 'JobRole': 'Laboratory Technician', 'JobSatisfaction': 3, 'MaritalStatus': 'Single', 'MonthlyIncome': 711, 'MonthlyRate': 2124, 'NumCompaniesWorked': 8, 'OverTime': 1, 'PercentSalaryHike': 8, 'PerformanceRating': 4, 'RelationshipSatisfaction': 3, 'StockOptionLevel': 0, 'TotalWorkingYears': 2, 'TrainingTimesLastYear': 0, 'WorkLifeBalance': 3, 'YearsAtCompany': 2, 'YearsInCurrentRole': 1, 'YearsSinceLastPromotion': 0, 'YearsWithCurrManager': 1}])\n",
    "\n",
    "\n",
    "# converts the sample to JSON string\n",
    "sample = pd.DataFrame.to_json(sample)\n",
    "\n",
    "# deserializes sample to a python object \n",
    "sample = json.loads(sample)\n",
    "\n",
    "# serializes sample to JSON formatted string as expected by the scoring script\n",
    "sample = json.dumps({\"data\":sample})\n",
    "\n",
    "prediction = aks_service.run(sample)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Setup role based access for your workspace\n",
    "\n",
    "Follow the steps [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-assign-roles) to enable custom roles on your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Monitoring and alerts\n",
    "\n",
    "Monitor e2e ML lifecycle and setup alerts using Azure Monitor. Check metrics section under AML workspace in Azure Portal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AzureML]",
   "language": "python",
   "name": "conda-env-AzureML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
